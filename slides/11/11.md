title: NPFL122, Lecture 11
class: title, langtech, cc-by-nc-sa
# MuZero, PlaNet,<br>ST and Gumbel-Softmax

## Milan Straka

### December 13, 2021

---
section: MuZero
# MuZero

The MuZero algorithm extends the AlphaZero by a **trained model**, alleviating
the requirement for a known MDP dynamics. It is evaluated both on board games
and on the Atari domain.

~~~
At each time-step $t$, for each of $1 ≤ k ≤ K$ steps, a model $μ_θ$, with parameters $θ$, conditioned on past observations $o_1, …, o_t$ and future actions $a_{t+1}, …, a_{t+k}$, predicts three future quantities:
- the policy $→p^k_t ≈ π(a_{t+k+1} | o_1, …, o_t, a_{t+1}, …, a_{t+k})$,
~~~
- the value function $v^k_t ≈ 𝔼\big[u_{t+k+1} + γ u_{t+k+2} + … | o_1, …, o_t, a_{t+1}, …, a_{t+k}\big]$,
~~~
- the immediate reward $r^k_t ≈ u_{t+k}$,

where $u_i$ are the observed rewards and $π$ is the behaviour policy.

---
# MuZero

At each time-step $t$ (omitted from now on for simplicity), the model is composed of three components,
a **representation** function, a **dynamics** function and a **prediction** function.

~~~
- The dynamics function, $r^k, s^k = g_θ(s^{k-1}, a^k)$, simulates the MDP
  dynamics and predicts an immediate reward $r^k$ and an internal state $s^k$.
  The internal state has no explicit semantics, its only goal is to accurately
  predict rewards, values and policies.

~~~
- The prediction function $→p^k, v^k = f_θ(s^k)$, computes the policy and value
  function, similarly as in AlphaZero.

~~~
- The representation function, $s^0 = h_θ(o_1, …, o_t)$, generates an internal
  state encoding the past observations.

---
# MuZero

![w=100%](muzero_overview.svgz)

---
# MuZero – MCTS

The MCTS algorithm is very similar to the one used in AlphaZero, only the
trained model is used. It produces a policy $π_t$ and value estimate $ν_t$.

~~~
- All actions, including the invalid ones, are allowed at any time, except at
  the root, where the invalid actions (available from the current state) are
  disallowed.

~~~
- No states are consider terminal during the search.

~~~
- During the backup phase, we consider a general discounted bootstrapped return
  $$G_k = ∑\nolimits_{t=0}^{l-k-1} γ^t r_{k+1+t} + γ^{l-k} v_l.$$

~~~
  Furthermore, the expected return is generally unbounded. Therefore, MuZero
  normalize the Q-value estimates to $[0, 1]$ range by using minimum and maximum
  values observed in the search tree until now:
  $$Q̄(s, a) = \frac{Q(s, a) - \min_{s',a' ∈ \mathrm{Tree}} Q(s', a')}{\max_{s',a' ∈ \mathrm{Tree}} Q(s', a') - \min_{s',a' ∈ \mathrm{Tree}} Q(s', a')}.$$

---
# MuZero – Action Selection

To select a move, we employ a MCTS algorithm and then sample
an action the obtained policy, $a_{t+1} ∼ π_t$.

~~~
For games, the same strategy of sampling the actions $a_t$ is used. In the Atari
domain, the actions are sampled according to visit counts for the whole episode,
but with a given temperature $T$:
$$π(a|s) = \frac{N(s, a)^{1/T}}{∑_b N(s, b)^{1/T}},$$
where $T$ is decayed during training – for first 500k steps it is 1, for the
next 250k steps it is 0.5 and for the last 250k steps it is 0.25.

~~~
While for the board games 800 simulations are used during MCTS, only 50 are used
for Atari.

~~~
In case of Atari, the replay buffer consists of 125k sequences of 200 actions.

---
# MuZero – Training

During training, we utilize a sequence of $K$ moves. We estimate the return
using bootstrapping as $z_t = u_{t+1} + γ u_{t+2} + … + γ^{n-1} u_{t+n} + γ^n ν_{t+n}$.
The values $K=5$ and $n=10$ are used in the paper.

~~~
The loss is then composed of the following components:
$$𝓛_t(θ) = ∑_{k=0}^K 𝓛^r (u_{t+k}, r_t^k) + 𝓛^v(z_{t+k}, v^k_t) + 𝓛^p(π_{t+k}, →p^k_t) + c \|\theta\|^2.$$

~~~
Note that in Atari, rewards are scaled by $\sign(x)\big(\sqrt{|x| + 1} - 1\big) + εx$ for $ε=10^{-3}$,
and authors utilize a cross-entropy loss with 601 categories for values $-300, …, 300$, which they claim
to be more stable.

~~~
Furthermore in Atari, the discount factor $γ=0.997$ is used and the replay buffer elements
are sampled according to prioritized replay and importance sampling is used to
account for changing the sampling distribution.

---
# MuZero

$$\begin{aligned}
&\text{Model} \\
&\left.
\begin{array}{ll}
s^0 &= h_θ(o_1, ..., o_t) \\
r^k, s^k &= g_θ(s^{k-1}, a^k) \\
→p^k, v^k &= f_θ(s^k)
\end{array}
\right\} \;\;
→p^k, v^k, r^k = μ_θ(o_1, ..., o_t, a^1, ..., a^k)\\
\\
&\text{Search}\\
ν_t, π_t &= MCTS(s^0_t, μ_θ) \\
a_t &∼ π_t
\end{aligned}$$

---
# MuZero

$$\begin{aligned}
&\text{Learning Rule} \\
→p^k_t, v^k_t, r^k_t &= μ_θ(o_1, …, o_t, a_{t+1}, ..., a_{t+k}) \\
z_t &= \left\{
\begin{array}{lr}
u_T
& \text{ for games } \\
u_{t+1} + γ u_{t+2} + ... + γ^{n-1} u_{t+n} + γ^n ν_{t+n}
& \text{ for general MDPs }
\end{array}
\right. \\
𝓛_t(θ) &= ∑_{k=0}^K 𝓛^r (u_{t+k}, r_t^k) + 𝓛^v(z_{t+k}, v^k_t) + 𝓛^p(π_{t+k}, →p^k_t)  + c \|θ\|^2 \\
&\text{Losses} \\
𝓛^r(u, r) &= \left\{
\begin{array}{lr}
0 & \text{ for games } \\
→φ(u)^T \log →r & \text{ for general MDPs }
\end{array}
\right. \\
𝓛^v(z, q) &= \left\{
\begin{array}{lr}
(z - q)^2 & \text{ for games } \\
→φ(z)^T \log →q & \text{ for general MDPs }
\end{array}
\right. \\
𝓛^p(π, p) &= →π^T \log →p
\end{aligned}$$

---
# MuZero – Evaluation

![w=100%](muzero_evaluation.svgz)

---
# MuZero – Atari Results

![w=100%](muzero_atari.svgz)

~~~
MuZero Reanalyze is optimized for greater sample efficiency. It revisits
past trajectories using the network with the latest parameters (using
the fresh policy in 80\% of the training steps).

---
# MuZero – Planning Ablations

![w=65%,h=center](muzero_planning_ablations.svgz)

---
# MuZero – Planning Ablations

![w=67%,h=center](muzero_planning_ablations_2.svgz)

---
# MuZero – Detailed Atari Results

![w=78%,h=center](muzero_atari_detailed_1.svgz)

---
# MuZero – Detailed Atari Results

![w=78%,h=center](muzero_atari_detailed_2.svgz)

---
section: PlaNet
# PlaNet

In Nov 2018, an interesting paper from D. Hafner et al. proposed
a **Deep Planning Network (PlaNet)**, which is a model-based agent
that learns the MDP dynamics from pixels and then chooses actions
using a CEM planner using a compact latent space.

~~~
The PlaNet is evaluated on selected tasks from the DeepMind control suite
![w=100%,mh=50%,v=middle](planet_environments.svgz)

---
# PlaNet

The partially observable MDPs are considered in PlaNet, that follow the
stochastic dynamics:
$$\begin{aligned}
\textrm{transition function:} && s_t &∼p(s_t|s_{t-1},a_{t-1}), \\
\textrm{observation function:} && o_t &∼p(o_t|s_t), \\
\textrm{reward function:} && r_t &∼p(r_t|s_t), \\
\textrm{policy:} && a_t &∼p(a_t|o_{≤t},a_{< t}).
\end{aligned}$$

~~~
The main goal is to train the first three – the transition function, the
observation function and the reward function.

---
# PlaNet – Data Collection

![w=32%,f=left](planet_algorithm.svgz)

Because an untrained agent will most likely not cover all needed environment
states, we need to iteratively collect new experience and train the model.
The authors propose $S=5$, $C=100$, $B=50$, $L=50$, $R$ between 2 and 8.

~~~
For planning, CEM algorithm capable of solving all tasks with a true model is
used; $H=12$, $I=10$, $J=1000$, $K=100$.

![w=85%,mw=66%,h=center](planet_planner.svgz)

---
# PlaNet – Latent Dynamics

![w=47%,f=right](planet_rssm.svgz)

First let us consider a typical latent-space model, consisting of
$$\begin{aligned}
\textrm{transition function:} && s_t &∼p(s_t|s_{t-1},a_{t-1}), \\
\textrm{observation function:} && o_t &∼p(o_t|s_t), \\
\textrm{reward function:} && r_t &∼p(r_t|s_t).
\end{aligned}$$

~~~
The transition model is Gaussian with mean and variance predicted by a network,
the observation model is Gaussian with identity covariance and mean predicted by
a deconvolutional network, and the reward model is a scalar Gaussian with unit
variance and mean predicted by a neural network.

~~~
To train such a model, we turn to variational inference and use an
encoder $q(s_{1:T}|o_{1:T},a_{1:T})=∏_{t=1}^T q(s_t|s_{t-1},a_{t-1},o_t)$,
which is a Gaussian with mean and variance predicted by a convolutional neural
network.

---
# PlaNet – Training Objective

Using the encoder, we obtain the following variational lower bound on the
log-likelihood of the observations (for rewards the bound is analogous):
$$\begin{aligned}
  &\log p(o_{1:T}|a_{1:T}) \\
  &\quad = \log ∫ ∏_t p(s_t|s_{t-1},a_{t-1}) p(o_t|s_t)\d s_{1:T} \\
  &\quad ≥ ∑_{t=1}^T \Big(
    \underbrace{𝔼_{q(s_t|o_{\leq t},a_{< t})} \log p(o_t|s_t)}_\textrm{reconstruction} -
    \underbrace{𝔼_{q(s_{t-1}|o_{≤ t-1},a_{<t-1})} D_\textrm{KL}\big(q(s_t|o_{≤ t},a_{< t}) \| p(s_t|s_{t-1},a_{t-1})\big)}_\textrm{complexity}
  \Big).
\end{aligned}$$

~~~
We evaluate the expectations using a single sample and use the reparametrization
trick to allow backpropagation through the sampling.

---
# PlaNet – Training Objective Derivation

To derive the training objective, we employ importance sampling and the Jensen’s
inequality:

$$\begin{aligned}
 &\log p(o_{1:T}|a_{1:T}) \\
 &\quad= \log 𝔼_{p(s_{1:T}|a_{1:T})} ∏_{t=1}^T p(o_t|s_t) \\
 &\quad= \log 𝔼_{q(s_{1:T}|o_{1:T},a_{1:T})} ∏_{t=1}^T p(o_t|s_t) p(s_t|s_{t-1},a_{t-1}) / q(s_t|o_{≤ t},a_{< t}) \\
 &\quad≥ 𝔼_{q(s_{1:T}|o_{1:T},a_{1:T})} ∑_{t=1}^T \log p(o_t|s_t) + \log p(s_t|s_{t-1},a_{t-1}) - \log q(s_t|o_{≤ t},a_{< t}) \\
 &\quad= ∑_{t=1}^T \Big(
    \underbrace{𝔼_{q(s_t|o_{\leq t},a_{< t})} \log p(o_t|s_t)}_\textrm{reconstruction} -
    \underbrace{𝔼_{q(s_{t-1}|o_{≤ t-1},a_{<t-1})} D_\textrm{KL}\big(q(s_t|o_{≤ t},a_{< t}) \| p(s_t|s_{t-1},a_{t-1})\big)}_\textrm{complexity}
  \Big).
\end{aligned}$$

---
# PlaNet – Recurrent State-Space Model

The purely stochastic transitions nevertheless struggle to store information for
multiple timesteps. Therefore, the authors propose to include a deterministic
path to the model, obtaining the **recurrent state-space model (RSSM)**:
![w=55%,h=center](planet_rssm.svgz)

~~~
$$\begin{aligned}
\textrm{deterministic state model:} && h_t &= f(h_{t-1}, s_{t-1}, a_{t-1}), \\
\textrm{stochastic state function:} && s_t &∼p(s_t|h_t), \\
\textrm{observation function:} && o_t &∼p(o_t|h_t, s_t), \\
\textrm{reward function:} && r_t &∼p(r_t|h_t, s_t), \\
\textrm{encoder:} && q_t &∼q(s_t|h_t, o_t).
\end{aligned}$$

---
# PlaNet – Results

![w=100%,v=middle](planet_results.svgz)

---
# PlaNet – Ablations

![w=85%,h=center](planet_ablations1.svgz)

---
# PlaNet – Ablations

![w=85%,h=center](planet_ablations2.svgz)

---
section: DiscreteLatentVariables
# Discrete Latent Variables

Consider that we would like to have discrete neurons on the hidden layer
of a neural network.

~~~
Note that on the output layer, we relaxed discrete prediction (i.e.,
an $\argmax$) with a continuous relaxation – $\softmax$. This way, we can
compute the derivatives, and also predict the most probable class.

~~~
However, on a hidden layer, we also need to _sample_ from the predicted
categorical distribution, and then backpropagate the gradients.

---
# Stochastic Gradient Estimators

![w=64%,h=center](stochastic_estimators.svgz)

---
# Stochastic Gradient Estimators

Several gradient estimators have been proposed:
~~~
- A REINFORCE-like gradient estimation.

  Using the identity $∇_{→θ} p_{→θ}(z) = p_{→θ}(z) ∇_{→θ} \log p_{→θ}(z)$, we
  obtain that
  $$∇_{→θ} 𝔼_z \big[f(z)\big] = 𝔼_z \big[f(z)∇_{→θ} \log p_{→θ}(z)\big].$$

~~~
  Analogously as before, we can also include the baseline for variance
  reduction, resulting in
  $$∇_{→θ} 𝔼_z \big[f(z)\big] = 𝔼_z \big[(f(z) - b) ∇_{→θ} \log p_{→θ}(z)\big].$$

~~~
- A **straight-through (ST)** estimator.

  The straight-through estimator has been proposed by Y. Bengio in 2013. It is
  a biased estimator, which assumes that $∇_{→θ}z ≈ 1$. Even if the bias can
  be considerable, it seems to work quite well in practice.

---
section: Gumbel-Softmax
# Gumbel-Softmax

The **Gumbel-Softmax** distribution was proposed independently in two papers
in Nov 2016 (under the name of **Concrete** distribution in the other paper).

~~~
It is a continuous distribution over the simplex (over categorical
distributions) that can approximate _sampling_ from a categorical distribution.

~~~
Let $z$ be a categorical variable with class probabilities $→p = (p_1, p_2, …, p_K)$.

~~~
The Gumbel-Max trick (based on a 1954 theorem from E. J. Gumbel) states that
we can draw samples $z ∼ →p$ using
$$z = \operatorname{one-hot}\Big(\argmax_i \big(g_i + \log p_i\big)\Big),$$
where $g_i$ are independent samples drawn from the $\operatorname{Gumbel}(0, 1)$
distribution.

To sample $g$ from the distribution $\operatorname{Gumbel}(0, 1)$, we can sample
$u ∼ U(0, 1)$ and then compute $g = -\log(-\log u)$.

---
# Gumbel-Softmax

To obtain a continuous distribution, we relax the $\argmax$ into a $\softmax$
with temperature $T$ as
$$z_i = \frac{e^{(g_i + \log p_i)/T}}{∑_j e^{(g_j + \log p_j)/T}}.$$

~~~
As the temperature $T$ goes to zero, the generated samples become one-hot and
therefore the Gumbel-Softmax distribution converges to the categorical
distribution $p(z)$.

![w=74%,h=center](gumbel_temperatures.svgz)

---
# Gumbel-Softmax Estimator

The Gumbel-Softmax distribution can be used to reparametrize the sampling of the
discrete variable using a fully differentiable estimator.

![w=54%,f=right](stochastic_estimators.svgz)

~~~
However, the resulting sample is not discrete, it only converges to a discrete
sample as the temperature $T$ goes to zero.

~~~
If it is a problem, we can combine the Gumbel-Softmax with a straight-through estimator,
obtaining ST Gumbel-Softmax, where we:
- discretize $→y$ as $z = \argmax →y$,
~~~
- assume $∇_{→θ}z ≈ ∇_{→θ}y$, or in other words, $\frac{∂z}{∂y} ≈ 1$.

---
# Gumbel-Softmax Estimator Results

![w=68%,h=center](gumbel_results.svgz)

![w=49%](gumbel_results_sbn.svgz)![w=49%](gumbel_results_vae.svgz)
