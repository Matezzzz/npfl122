title: NPFL122, Lecture 12
class: title, langtech, cc-by-nc-sa
# ST and Gumbel-Softmax

## Milan Straka

### December 20, 2021

---
section: DiscreteLatentVariables
# Discrete Latent Variables

Consider that we would like to have discrete neurons on the hidden layer
of a neural network.

~~~
Note that on the output layer, we relaxed discrete prediction (i.e.,
an $\argmax$) with a continuous relaxation – $\softmax$. This way, we can
compute the derivatives, and also predict the most probable class.

~~~
However, on a hidden layer, we also need to _sample_ from the predicted
categorical distribution, and then backpropagate the gradients.

---
# Stochastic Gradient Estimators

![w=64%,h=center](stochastic_estimators.svgz)

---
# Stochastic Gradient Estimators

Several gradient estimators have been proposed:
~~~
- A REINFORCE-like gradient estimation.

  Using the identity $∇_{→θ} p_{→θ}(z) = p_{→θ}(z) ∇_{→θ} \log p_{→θ}(z)$, we
  obtain that
  $$∇_{→θ} 𝔼_z \big[f(z)\big] = 𝔼_z \big[f(z)∇_{→θ} \log p_{→θ}(z)\big].$$

~~~
  Analogously as before, we can also include the baseline for variance
  reduction, resulting in
  $$∇_{→θ} 𝔼_z \big[f(z)\big] = 𝔼_z \big[(f(z) - b) ∇_{→θ} \log p_{→θ}(z)\big].$$

~~~
- A **straight-through (ST)** estimator.

  The straight-through estimator has been proposed by Y. Bengio in 2013. It is
  a biased estimator, which assumes that $∇_{→θ}z ≈ 1$. Even if the bias can
  be considerable, it seems to work quite well in practice.

---
section: Gumbel-Softmax
# Gumbel-Softmax

The **Gumbel-Softmax** distribution was proposed independently in two papers
in Nov 2016 (under the name of **Concrete** distribution in the other paper).

~~~
It is a continuous distribution over the simplex (over categorical
distributions) that can approximate _sampling_ from a categorical distribution.

~~~
Let $z$ be a categorical variable with class probabilities $→p = (p_1, p_2, …, p_K)$.

~~~
The Gumbel-Max trick (based on a 1954 theorem from E. J. Gumbel) states that
we can draw samples $z ∼ →p$ using
$$z = \operatorname{one-hot}\Big(\argmax_i \big(g_i + \log p_i\big)\Big),$$
where $g_i$ are independent samples drawn from the $\operatorname{Gumbel}(0, 1)$
distribution.

To sample $g$ from the distribution $\operatorname{Gumbel}(0, 1)$, we can sample
$u ∼ U(0, 1)$ and then compute $g = -\log(-\log u)$.

---
# Gumbel-Softmax

To obtain a continuous distribution, we relax the $\argmax$ into a $\softmax$
with temperature $T$ as
$$z_i = \frac{e^{(g_i + \log p_i)/T}}{∑_j e^{(g_j + \log p_j)/T}}.$$

~~~
As the temperature $T$ goes to zero, the generated samples become one-hot and
therefore the Gumbel-Softmax distribution converges to the categorical
distribution $p(z)$.

![w=74%,h=center](gumbel_temperatures.svgz)

---
# Gumbel-Softmax Estimator

The Gumbel-Softmax distribution can be used to reparametrize the sampling of the
discrete variable using a fully differentiable estimator.

![w=54%,f=right](stochastic_estimators.svgz)

~~~
However, the resulting sample is not discrete, it only converges to a discrete
sample as the temperature $T$ goes to zero.

~~~
If it is a problem, we can combine the Gumbel-Softmax with a straight-through estimator,
obtaining ST Gumbel-Softmax, where we:
- discretize $→y$ as $z = \argmax →y$,
~~~
- assume $∇_{→θ}z ≈ ∇_{→θ}y$, or in other words, $\frac{∂z}{∂y} ≈ 1$.

---
# Gumbel-Softmax Estimator Results

![w=68%,h=center](gumbel_results.svgz)

![w=49%](gumbel_results_sbn.svgz)![w=49%](gumbel_results_vae.svgz)
